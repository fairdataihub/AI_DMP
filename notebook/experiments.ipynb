{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0f6b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 10/08/2025\n"
     ]
    }
   ],
   "source": [
    "print (\"Start 10/08/2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d516c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 📦 Core Imports\n",
    "# ============================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================================\n",
    "# 🧠 LLM & Embeddings\n",
    "# ============================================\n",
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# ============================================\n",
    "# 📄 Document Processing\n",
    "# ============================================\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# ============================================\n",
    "# 📑 File Conversion\n",
    "# ============================================\n",
    "import pypandoc\n",
    "\n",
    "# ============================================\n",
    "# 🔐 Environment Setup\n",
    "# ============================================\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================\n",
    "# ⚙️ User Configurable Parameters\n",
    "# ============================================\n",
    "DATA_DIR = Path(\"data\")                    # Folder where your PDFs are located\n",
    "INDEX_DIR = Path(\"data/faiss_index\")       # Folder to save FAISS index\n",
    "EMBED_MODEL = \"nomic-embed-text\"           # Embedding model for RAG\n",
    "CHUNK_SIZE = 800                           # Token/text length per chunk\n",
    "CHUNK_OVERLAP = 120                        # Overlap between chunks\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0f897a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 📘 Function: Load PDFs\n",
    "# ============================================\n",
    "def load_pdfs_from_folder(folder: Path) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Load all PDFs from the specified folder and return a list of Document objects.\n",
    "    Each PDF is loaded page by page.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    folder = folder.resolve()\n",
    "\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"❌ Data folder not found: {folder}\")\n",
    "\n",
    "    pdf_files = sorted(folder.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"⚠️ No PDF files found in {folder}\")\n",
    "        return docs\n",
    "\n",
    "    print(f\"📚 Found {len(pdf_files)} PDF(s) in {folder}\")\n",
    "    for f in pdf_files:\n",
    "        print(f\"🔹 Loading: {f.name}\")\n",
    "        loader = PyPDFLoader(str(f))\n",
    "        file_docs = loader.load()\n",
    "        docs.extend(file_docs)\n",
    "\n",
    "    print(f\"✅ Loaded total {len(docs)} pages from all PDFs.\\n\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2466528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 🧩 Function: Split into Chunks\n",
    "# ============================================\n",
    "def split_into_chunks(docs, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"✅ Created {len(chunks)} text chunks from {len(docs)} pages.\")\n",
    "    print(\"🔹 Example chunk:\\n\", chunks[0].page_content[:300], \"...\\n\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89a926c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# 🧮 Function: Build & Save FAISS Index\n",
    "# ============================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def build_faiss_index(chunks, embed_model=EMBED_MODEL, index_dir=INDEX_DIR):\n",
    "    \"\"\"\n",
    "    Build FAISS vector index from document chunks and save locally.\n",
    "    \"\"\"\n",
    "    index_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"🚀 Initializing embedding model: {embed_model}\")\n",
    "    embeddings = OllamaEmbeddings(model=embed_model)\n",
    "\n",
    "    print(\"⚙️ Building FAISS index ...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    vectorstore.save_local(str(index_dir))\n",
    "    print(f\"✅ FAISS vector store built and saved to: {index_dir.resolve()}\")\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "994ac8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Found 26 PDF(s) in C:\\Users\\Nahid\\DMP-RAG\\notebook\\data\n",
      "🔹 Loading: 1-Clinical andor MRI data from human research participants-NIMH.pdf\n",
      "🔹 Loading: 10-Clinical Data from Human Research Participants-NIDDK.pdf\n",
      "🔹 Loading: 11-Basic Research from a Non-Human Source Example-NIDDK.pdf\n",
      "🔹 Loading: 12-Secondary Data Analysis Example-NIDDK.pdf\n",
      "🔹 Loading: 13-Survey and Interview Example-NHGRI.pdf\n",
      "🔹 Loading: 14-Human Clinical Trial Data-NICHD.pdf\n",
      "🔹 Loading: 15-Clinical data from human research participants-NIA.pdf\n",
      "🔹 Loading: 16-Survey, interview, and biological data (tiered access)-NIA.pdf\n",
      "🔹 Loading: 17-Non-human data (primates)-NIA.pdf\n",
      "🔹 Loading: 18-Secondary data analysis-NIA.pdf\n",
      "🔹 Loading: 19-Survey and interview data-NIA.pdf\n",
      "🔹 Loading: 2-Genomic data from human research participants-NIMH.pdf\n",
      "🔹 Loading: 20-Human clinical and genomic data-NIA.pdf\n",
      "🔹 Loading: 21-Non-human data (rodents)-NIA.pdf\n",
      "🔹 Loading: 22-Clinical data (human biospecimens)-NIA.pdf\n",
      "🔹 Loading: 23-Drug discovery including intellectual property-NIA.pdf\n",
      "🔹 Loading: 24-HeLa Cell Whole Genome Sequence (DNA or RNA)-OD, NHGRI.pdf\n",
      "🔹 Loading: 25-Secondary Data Analysis on Data from Human Subjects-NIA.pdf\n",
      "🔹 Loading: 26-Analysis of social media posts-NCI.pdf\n",
      "🔹 Loading: 3-Genomic data from a non-human source-NIMH.pdf\n",
      "🔹 Loading: 4-Secondary data analysis-NIMH.pdf\n",
      "🔹 Loading: 5-Human genomic data-NHGRI.pdf\n",
      "🔹 Loading: 6-Technology development-NHGRI.pdf\n",
      "🔹 Loading: 7-Human clinical and genomics data-NICHD.pdf\n",
      "🔹 Loading: 8-Gene expression analysis data from non-human model organism (zebrafish)-NICHD.pdf\n",
      "🔹 Loading: 9-Human survey data-NICHD.pdf\n",
      "✅ Loaded total 75 pages from all PDFs.\n",
      "\n",
      "✅ Created 370 text chunks from 75 pages.\n",
      "🔹 Example chunk:\n",
      " DATA MANAGEMENT AND SHARING PLAN \n",
      "An example from an application proposing to collect clinical and MRI data from human subjects. \n",
      "If any of the proposed research in the application involves the generation of scientific data, this application is subject to the NIH Policy \n",
      "for Data Management and Shar ...\n",
      "\n",
      "🚀 Initializing embedding model: nomic-embed-text\n",
      "⚙️ Building FAISS index ...\n",
      "✅ FAISS vector store built and saved to: C:\\Users\\Nahid\\DMP-RAG\\notebook\\data\\faiss_index\n",
      "🎯 Stage 1 complete: documents indexed and ready for retrieval.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🧭 Pipeline Execution\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    raw_docs = load_pdfs_from_folder(DATA_DIR)\n",
    "    if not raw_docs:\n",
    "        print(\"❌ No documents loaded — please add PDFs to the 'data' folder.\")\n",
    "    else:\n",
    "        chunks = split_into_chunks(raw_docs)\n",
    "        vectorstore = build_faiss_index(chunks)\n",
    "        print(\"🎯 Stage 1 complete: documents indexed and ready for retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21f56b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading FAISS index from: C:\\Users\\Nahid\\DMP-RAG\\notebook\\data\\faiss_index\n",
      "✅ Retriever ready (top_k=3) using embedding: nomic-embed-text\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nahid\\AppData\\Local\\Temp\\ipykernel_26536\\3607509811.py:41: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(test_query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Retrieved 3 relevant chunks.\n",
      "🧩 Sample chunk preview:\n",
      "\n",
      "DATA MANAGEMENT AND SHARING PLAN \n",
      "An example from an application proposing to collect clinical and MRI data from human subjects. \n",
      "If any of the proposed research in the application involves the generation of scientific data, this application is subject to the NIH Policy \n",
      "for Data Management and Sharing and requires submission of a Data Management and Sharing Plan. If the proposed research in the \n",
      " ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🧩 Load FAISS Index + Create Retriever\n",
    "# ============================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# ---- Paths and Parameters ----\n",
    "INDEX_DIR   = Path(\"data/faiss_index\")   # same folder where Stage 1 stored your index\n",
    "EMBED_MODEL = \"nomic-embed-text\"         # must match the model used to build the index\n",
    "TOP_K       = 3                          # how many relevant chunks to retrieve for each query\n",
    "\n",
    "def load_retriever(index_dir=INDEX_DIR, embed_model=EMBED_MODEL, k=TOP_K):\n",
    "    \"\"\"\n",
    "    Loads the FAISS index built earlier and creates a retriever object\n",
    "    for Retrieval-Augmented Generation (RAG).\n",
    "    \"\"\"\n",
    "    print(\"🚀 Loading FAISS index from:\", index_dir.resolve())\n",
    "\n",
    "    # Initialize embedding model\n",
    "    embeddings = OllamaEmbeddings(model=embed_model)\n",
    "\n",
    "    # Load FAISS vector store from local folder\n",
    "    vectorstore = FAISS.load_local(\n",
    "        str(index_dir),\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "    # Create retriever for semantic search\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    print(f\"✅ Retriever ready (top_k={k}) using embedding: {embed_model}\\n\")\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# ---- Quick Test ----\n",
    "retriever = load_retriever()\n",
    "\n",
    "# Optional sanity-check: try one retrieval\n",
    "test_query = \"How should NIH Data Management Plans handle genomic data and consent?\"\n",
    "docs = retriever.get_relevant_documents(test_query)\n",
    "print(f\"🔎 Retrieved {len(docs)} relevant chunks.\")\n",
    "if docs:\n",
    "    print(\"🧩 Sample chunk preview:\\n\")\n",
    "    print(docs[0].page_content[:400], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f1bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel file loaded successfully!\n",
      "📄 Loaded 26 rows from inputs\\inputs.xlsx\n",
      "✅ DMP template loaded successfully!\n",
      "🚀 Loading FAISS index from: C:\\Users\\Nahid\\DMP-RAG\\notebook\\data\\faiss_index\n",
      "✅ Retriever ready (top_k=3) using embedding: nomic-embed-text\n",
      "\n",
      "🔗 RAG chain initialized using model: llama4\n",
      "\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Mental Health (NIMH)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Institute_of_Mental_Health_NIMH.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Institute_of_Mental_Health_NIMH.docx\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Mental Health (NIMH)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Institute_of_Mental_Health_NIMH.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Institute_of_Mental_Health_NIMH.docx\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Mental Health (NIMH)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Institute_of_Mental_Health_NIMH.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Institute_of_Mental_Health_NIMH.docx\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Mental Health (NIMH)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Institute_of_Mental_Health_NIMH.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Institute_of_Mental_Health_NIMH.docx\n",
      "\n",
      "🧩 Generating DMP for: Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD)\n",
      "💾 Saved: outputs\\markdown\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.md\n",
      "📄 Converted: outputs\\docx\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.docx\n",
      "\n",
      "🧩 Generating DMP for: Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD)\n",
      "💾 Saved: outputs\\markdown\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.md\n",
      "📄 Converted: outputs\\docx\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.docx\n",
      "\n",
      "🧩 Generating DMP for: Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD)\n",
      "💾 Saved: outputs\\markdown\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.md\n",
      "📄 Converted: outputs\\docx\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.docx\n",
      "\n",
      "🧩 Generating DMP for: The National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) \n",
      "💾 Saved: outputs\\markdown\\DMP_The_National_Institute_of_Diabetes_and_Digestive_and_Kidney_Diseases_NIDDK_.md\n",
      "📄 Converted: outputs\\docx\\DMP_The_National_Institute_of_Diabetes_and_Digestive_and_Kidney_Diseases_NIDDK_.docx\n",
      "\n",
      "🧩 Generating DMP for: National Human Genome Research Institute (NHGRI)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Human_Genome_Research_Institute_NHGRI.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Human_Genome_Research_Institute_NHGRI.docx\n",
      "\n",
      "🧩 Generating DMP for: National Human Genome Research Institute (NHGRI)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Human_Genome_Research_Institute_NHGRI.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Human_Genome_Research_Institute_NHGRI.docx\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Institute_of_Diabetes_and_Digestive_and_Kidney_Diseases_NIDDK.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Institute_of_Diabetes_and_Digestive_and_Kidney_Diseases_NIDDK.docx\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Institute_of_Diabetes_and_Digestive_and_Kidney_Diseases_NIDDK.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Institute_of_Diabetes_and_Digestive_and_Kidney_Diseases_NIDDK.docx\n",
      "\n",
      "🧩 Generating DMP for: National Human Genome Research Institute (NHGRI)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Human_Genome_Research_Institute_NHGRI.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Human_Genome_Research_Institute_NHGRI.docx\n",
      "\n",
      "🧩 Generating DMP for: Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD)\n",
      "💾 Saved: outputs\\markdown\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.md\n",
      "📄 Converted: outputs\\docx\\DMP_Eunice_Kennedy_Shriver_National_Institute_of_Child_Health_and_Human_Development_NICHD.docx\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Againg (NIA)\n",
      "💾 Saved: outputs\\markdown\\DMP_National_Institute_of_Againg_NIA.md\n",
      "📄 Converted: outputs\\docx\\DMP_National_Institute_of_Againg_NIA.docx\n",
      "\n",
      "🧩 Generating DMP for: National Institute of Againg (NIA)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 📦 Core Imports\n",
    "# ============================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================================\n",
    "# 🧠 LangChain & Ollama Components\n",
    "# ============================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# ============================================\n",
    "# ⚙️ Configuration\n",
    "# ============================================\n",
    "load_dotenv()\n",
    "\n",
    "INDEX_DIR   = Path(\"data/faiss_index\")\n",
    "EXCEL_PATH  = Path(\"inputs/inputs.xlsx\")\n",
    "TEMPLATE_PATH = Path(\"inputs/dmp-template.md\")\n",
    "\n",
    "OUTPUT_MD   = Path(\"outputs/markdown\")\n",
    "OUTPUT_DOCX = Path(\"outputs/docx\")\n",
    "\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL   = \"llama4\"\n",
    "TOP_K       = 3\n",
    "\n",
    "# ============================================\n",
    "# 📁 Function3: Create a Directory if it Doesn't Exist\n",
    "# ============================================\n",
    "def create_folder(folderpath):\n",
    "    \"\"\"Creates a folder at the specified path if it doesn't already exist.\"\"\"\n",
    "    if not os.path.exists(folderpath):\n",
    "        os.makedirs(folderpath)\n",
    "\n",
    "# ============================================\n",
    "# 📄 Function4: Save Markdown Files\n",
    "# ============================================\n",
    "def save_md(folderpath, filename, response):\n",
    "    \"\"\"Saves a given response as a Markdown (.md) file.\"\"\"\n",
    "    create_folder(folderpath)\n",
    "    filepath = os.path.join(folderpath, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response)\n",
    "    print(\"💾 Saved:\", filepath)\n",
    "\n",
    "# ============================================\n",
    "# 📑 Function5: Convert Markdown to DOCX\n",
    "# ============================================\n",
    "def md_to_docs(md_filepath, docx_folderpath, docx_filename):\n",
    "    \"\"\"Converts a Markdown file to a Word (.docx) document using pypandoc.\"\"\"\n",
    "    create_folder(docx_folderpath)\n",
    "    docx_filepath = os.path.join(docx_folderpath, docx_filename)\n",
    "    pypandoc.convert_file(md_filepath, \"docx\", outputfile=docx_filepath)\n",
    "    print(\"📄 Converted:\", docx_filepath)\n",
    "\n",
    "# ============================================\n",
    "# 📊 Step 1: Load Excel Inputs and DMP Template\n",
    "# ============================================\n",
    "if EXCEL_PATH.exists():\n",
    "    df = pd.read_excel(EXCEL_PATH)\n",
    "    print(\"✅ Excel file loaded successfully!\")\n",
    "    print(f\"📄 Loaded {len(df)} rows from {EXCEL_PATH}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"❌ The Excel file '{EXCEL_PATH}' was not found.\")\n",
    "\n",
    "if TEMPLATE_PATH.exists():\n",
    "    with open(TEMPLATE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "        dmp_template_text = file.read()\n",
    "    print(\"✅ DMP template loaded successfully!\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"❌ The template file '{TEMPLATE_PATH}' was not found.\")\n",
    "\n",
    "# ============================================\n",
    "# 🧩 Step 2: Load FAISS Index + Create Retriever\n",
    "# ============================================\n",
    "def load_retriever(index_dir=INDEX_DIR, embed_model=EMBED_MODEL, k=TOP_K):\n",
    "    \"\"\"Loads the FAISS index and creates a retriever object.\"\"\"\n",
    "    print(\"🚀 Loading FAISS index from:\", index_dir.resolve())\n",
    "    embeddings = OllamaEmbeddings(model=embed_model)\n",
    "    vectorstore = FAISS.load_local(\n",
    "        str(index_dir), embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    print(f\"✅ Retriever ready (top_k={k}) using embedding: {embed_model}\\n\")\n",
    "    return retriever\n",
    "\n",
    "retriever = load_retriever()\n",
    "\n",
    "# ============================================\n",
    "# 🧠 Step 3: Build the RAG Chain\n",
    "# ============================================\n",
    "def build_rag_chain(retriever, llm_model=LLM_MODEL):\n",
    "    \"\"\"Builds a Retrieval-Augmented Generation (RAG) chain.\"\"\"\n",
    "    llm = Ollama(model=llm_model)\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert biomedical data steward and grant writer.\n",
    "    Your goal is to create a high-quality NIH Data Management and Sharing Plan (DMSP)\n",
    "    based on the question and the retrieved NIH context.\n",
    "\n",
    "    Answer the question based on the context provided below.\n",
    "    If the context does not contain sufficient information, respond with:\n",
    "    \"I do not have enough information about this.\"\n",
    "\n",
    "    ----\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "\n",
    "    print(f\"🔗 RAG chain initialized using model: {llm_model}\\n\")\n",
    "    return rag_chain\n",
    "\n",
    "rag_chain = build_rag_chain(retriever)\n",
    "\n",
    "# ============================================\n",
    "# 🧩 Step 4: Generate Queries and Run RAG\n",
    "# ============================================\n",
    "def generate_query(row):\n",
    "    \"\"\"Builds a DMP-specific query from Excel row data.\"\"\"\n",
    "    query_initiate = (\n",
    "        \"You are an expert biomedical grant writer and data steward. \"\n",
    "        \"Create a Data Management and Sharing Plan (DMSP) for a grant proposal being submitted to the NIH.\"\n",
    "    )\n",
    "    query_funding_agency = f\"Specifically targeting the {row['institute']}.\"\n",
    "    query_element1a = f\"Here are the details about the data to be collected:\\n{row['element_1A']}\\n\"\n",
    "\n",
    "    if \"yes\" in str(row.get(\"isHumanStudy\", \"\")).lower():\n",
    "        query_consent_type = (\n",
    "            f\"This proposal includes a study involving human participants. \"\n",
    "            f\"{row.get('consentDescription', '')}.\"\n",
    "        )\n",
    "    else:\n",
    "        query_consent_type = \"\"\n",
    "\n",
    "    # ✅ FIXED: use the loaded Markdown template text, not a filename\n",
    "    query_template = (\n",
    "        \"Provide the result using exactly this markdown format template of the DMSP provided by the NIH \"\n",
    "        \"without changing it (keep all the titles and sections as is):\\n\"\n",
    "        + dmp_template_text\n",
    "    )\n",
    "\n",
    "    return \" \".join([\n",
    "        query_initiate,\n",
    "        query_funding_agency,\n",
    "        query_element1a,\n",
    "        query_consent_type,\n",
    "        query_template\n",
    "    ])\n",
    "\n",
    "# ============================================\n",
    "# 🚀 Step 5: Generate and Save DMPs\n",
    "# ============================================\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"\\n🧩 Generating DMP for: {row['institute']}\")\n",
    "    query = generate_query(row)\n",
    "\n",
    "    # --- Run through RAG pipeline ---\n",
    "    response = rag_chain.invoke(query)\n",
    "\n",
    "    # --- Save Markdown ---\n",
    "    filename_md = f\"DMP_{row['institute'].replace(' ', '_').replace('(', '').replace(')', '')}.md\"\n",
    "    md_path = OUTPUT_MD / filename_md\n",
    "    save_md(str(OUTPUT_MD), filename_md, response)\n",
    "\n",
    "    # --- Convert Markdown → DOCX ---\n",
    "    filename_docx = filename_md.replace(\".md\", \".docx\")\n",
    "    md_to_docs(str(md_path), str(OUTPUT_DOCX), filename_docx)\n",
    "\n",
    "print(\"\\n🎯 All NIH DMPs generated, saved as Markdown, and converted to DOCX!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374ab14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d78eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f8d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bfb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46ea97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c0751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

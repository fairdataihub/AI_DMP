{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ \n",
    "# STEP 3 â€” Build or Load Hybrid (Semantic + Lexical) Retriever (RAG-Version 6)\n",
    "# ============================================\n",
    "\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "import lancedb\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Parameters ---\n",
    "TOP_K = 8\n",
    "EMBED_MODEL = \"mixedbread-ai/mxbai-embed-large-v1\"  # Dense semantic embeddings\n",
    "INDEX_DIR = Path(\"data/index/lancedb_v6_hybrid\")\n",
    "\n",
    "# --- Auto-detect device ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ’» Using device: {DEVICE.upper()}\")\n",
    "\n",
    "# --- Auto-adjust batch size by VRAM ---\n",
    "def auto_batch_size():\n",
    "    if DEVICE != \"cuda\":\n",
    "        return 16\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if vram_gb >= 22:\n",
    "        return 128\n",
    "    elif vram_gb >= 12:\n",
    "        return 64\n",
    "    else:\n",
    "        return 32\n",
    "\n",
    "BATCH_SIZE = auto_batch_size()\n",
    "print(f\"ðŸ§  GPU VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"âš™ï¸ Batch size set to: {BATCH_SIZE}\")\n",
    "\n",
    "# --- Initialize dense embedding model ---\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": DEVICE},\n",
    "    encode_kwargs={\"batch_size\": BATCH_SIZE}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Function: Build or Load LanceDB Semantic Index\n",
    "# ------------------------------------------------------------\n",
    "def build_or_load_lancedb_index(index_dir=INDEX_DIR, chunks=None):\n",
    "    \"\"\"\n",
    "    Builds or loads a LanceDB semantic index for hybrid dense + lexical retrieval.\n",
    "    \"\"\"\n",
    "    index_dir.mkdir(parents=True, exist_ok=True)\n",
    "    db = lancedb.connect(str(index_dir))\n",
    "\n",
    "    # --- Load existing index if available ---\n",
    "    if \"documents\" in db.table_names():\n",
    "        print(\"ðŸ“¦ Existing LanceDB semantic index found. Loading from disk...\")\n",
    "        table = db.open_table(\"documents\")\n",
    "        vectorstore = LanceDB(connection=db, table=table, embedding=embeddings)\n",
    "        print(\"âœ… LanceDB semantic index loaded successfully.\")\n",
    "        return vectorstore\n",
    "\n",
    "    # --- Validate chunks ---\n",
    "    if not chunks:\n",
    "        raise RuntimeError(\"âŒ No chunks provided. Please run Step 2 first.\")\n",
    "\n",
    "    total = len(chunks)\n",
    "    print(f\"ðŸ§± Building new LanceDB semantic index on {DEVICE.upper()}...\")\n",
    "    print(f\"ðŸ“Š Total chunks to embed: {total:,}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Create LanceDB vector store from documents ---\n",
    "    vectorstore = LanceDB.from_documents(\n",
    "        tqdm(chunks, desc=\"ðŸ”¢ Embedding text chunks\", ncols=100),\n",
    "        embedding=embeddings,\n",
    "        uri=str(index_dir),\n",
    "        table_name=\"documents\"\n",
    "    )\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"ðŸ’¾ Index saved to {index_dir}\")\n",
    "    print(f\"â±ï¸ Build completed in {duration/60:.2f} min ({duration:.1f} sec)\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Execute step â€” Build or Load Index\n",
    "# ------------------------------------------------------------\n",
    "vectorstore = build_or_load_lancedb_index(INDEX_DIR, chunks)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3.1 â€” Create Semantic and Lexical Retrievers\n",
    "# ------------------------------------------------------------\n",
    "print(\"âš™ï¸ Initializing hybrid retrievers (semantic + lexical)...\")\n",
    "semantic_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": TOP_K}\n",
    ")\n",
    "\n",
    "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3.2 â€” Combine Dense + Sparse Retrieval (Weighted Ensemble)\n",
    "# ------------------------------------------------------------\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[semantic_retriever, keyword_retriever],\n",
    "    weights=[0.7, 0.3]  # Î±semantic = 0.7, Î±keyword = 0.3\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Final Summary\n",
    "# ------------------------------------------------------------\n",
    "print(f\"âœ… Hybrid Retriever ready (LanceDB + BM25, Î±semantic=0.7, Î±keyword=0.3, top_k={TOP_K})\")\n",
    "print(\"ðŸ“ˆ Configuration Summary:\")\n",
    "print(f\"   â€¢ Embedding Model: {EMBED_MODEL}\")\n",
    "print(f\"   â€¢ Vector DB: LanceDB (semantic)\")\n",
    "print(f\"   â€¢ Lexical Model: BM25 (keyword)\")\n",
    "print(f\"   â€¢ Retrieval Mode: Hybrid Dense + Sparse Fusion\")\n",
    "print(f\"   â€¢ Top-k: {TOP_K}\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Device: {DEVICE.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ \n",
    "# STEP 3 â€” Build or Load Multi-Embedding Ensemble Index (RAG-Version 7)\n",
    "# ============================================\n",
    "\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "import lancedb\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Parameters ---\n",
    "TOP_K = 8\n",
    "INDEX_DIR = Path(\"data/index/lancedb_v7_ensemble\")\n",
    "\n",
    "# Embedding models (semantic, hybrid, instruction-aware)\n",
    "EMBED_MODELS = {\n",
    "    \"semantic\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"hybrid\": \"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "    \"instruction\": \"hkunlp/instructor-large\"\n",
    "}\n",
    "\n",
    "# --- Auto-detect device ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ’» Using device: {DEVICE.upper()}\")\n",
    "\n",
    "# --- Auto-adjust batch size based on VRAM ---\n",
    "def auto_batch_size():\n",
    "    if DEVICE != \"cuda\":\n",
    "        return 16\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if vram_gb >= 22:\n",
    "        return 128\n",
    "    elif vram_gb >= 12:\n",
    "        return 64\n",
    "    else:\n",
    "        return 32\n",
    "\n",
    "BATCH_SIZE = auto_batch_size()\n",
    "print(f\"ðŸ§  GPU VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"âš™ï¸ Batch size set to: {BATCH_SIZE}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Function: Build or Load a LanceDB Index for each embedding model\n",
    "# ------------------------------------------------------------\n",
    "def build_or_load_lancedb_index(model_name: str, model_label: str, chunks, base_dir=INDEX_DIR):\n",
    "    \"\"\"\n",
    "    Builds or loads a LanceDB index for a specific embedding model.\n",
    "    Each model stores its own embeddings to allow ensemble fusion later.\n",
    "    \"\"\"\n",
    "    index_path = base_dir / model_label\n",
    "    index_path.mkdir(parents=True, exist_ok=True)\n",
    "    db = lancedb.connect(str(index_path))\n",
    "\n",
    "    # --- Initialize embedding model ---\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": DEVICE},\n",
    "        encode_kwargs={\"batch_size\": BATCH_SIZE}\n",
    "    )\n",
    "\n",
    "    # --- Load existing index if available ---\n",
    "    if \"documents\" in db.table_names():\n",
    "        print(f\"ðŸ“¦ Existing LanceDB index ({model_label}) found â€“ loading...\")\n",
    "        table = db.open_table(\"documents\")\n",
    "        return LanceDB(connection=db, table=table, embedding=embeddings)\n",
    "\n",
    "    # --- Validate chunks ---\n",
    "    if not chunks:\n",
    "        raise RuntimeError(\"âŒ No chunks provided. Please run Step 2 first.\")\n",
    "\n",
    "    total = len(chunks)\n",
    "    print(f\"ðŸ§± Building LanceDB index for '{model_label}' on {DEVICE.upper()}...\")\n",
    "    print(f\"ðŸ“Š Total chunks to embed: {total:,}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    vectorstore = LanceDB.from_documents(\n",
    "        tqdm(chunks, desc=f\"ðŸ”¢ Embedding chunks ({model_label})\", ncols=100),\n",
    "        embedding=embeddings,\n",
    "        uri=str(index_path),\n",
    "        table_name=\"documents\",\n",
    "    )\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"ðŸ’¾ Saved LanceDB index ({model_label}) â†’ {index_path}\")\n",
    "    print(f\"â±ï¸ Build completed in {duration/60:.2f} min ({duration:.1f} sec)\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3.1 â€“ Build or Load all VectorStores\n",
    "# ------------------------------------------------------------\n",
    "vectorstores = {}\n",
    "for label, model_name in EMBED_MODELS.items():\n",
    "    vectorstores[label] = build_or_load_lancedb_index(model_name, label, chunks)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3.2 â€“ Create Individual Retrievers\n",
    "# ------------------------------------------------------------\n",
    "retrievers = {\n",
    "    label: store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": TOP_K})\n",
    "    for label, store in vectorstores.items()\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3.3 â€“ Combine via Ensemble Retriever\n",
    "# ------------------------------------------------------------\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=list(retrievers.values()),\n",
    "    weights=[0.4, 0.3, 0.3]  # semantic = 0.4, hybrid = 0.3, instruction = 0.3\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Summary\n",
    "# ------------------------------------------------------------\n",
    "print(f\"âœ… Retriever ready (Multi-Embedding Ensemble â€“ semantic=0.4, hybrid=0.3, instruction=0.3, top_k={TOP_K})\")\n",
    "print(\"ðŸ“ˆ Configuration Summary:\")\n",
    "print(f\"   â€¢ Vector DB: LanceDB (3 indexes)\")\n",
    "print(f\"   â€¢ Embedding Models: {', '.join(EMBED_MODELS.values())}\")\n",
    "print(f\"   â€¢ Fusion Weights: [0.4, 0.3, 0.3]\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Device: {DEVICE.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ee117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ \n",
    "# STEP 3 â€” Build or Load LanceDB Index with Dynamic Chunk Fusion (RAG-Version 8)\n",
    "# ============================================\n",
    "\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import lancedb\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Parameters ---\n",
    "TOP_K = 6  # fewer but richer context chunks\n",
    "EMBED_MODEL = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "INDEX_DIR = Path(\"data/index/lancedb_v8_chunkfusion\")\n",
    "\n",
    "# --- Device setup ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ’» Using device: {DEVICE.upper()}\")\n",
    "\n",
    "# --- Auto-adjust batch size based on VRAM ---\n",
    "def auto_batch_size():\n",
    "    if DEVICE != \"cuda\":\n",
    "        return 16\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if vram_gb >= 22:\n",
    "        return 128\n",
    "    elif vram_gb >= 12:\n",
    "        return 64\n",
    "    else:\n",
    "        return 32\n",
    "\n",
    "BATCH_SIZE = auto_batch_size()\n",
    "print(f\"ðŸ§  GPU VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"âš™ï¸ Batch size set to: {BATCH_SIZE}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Function: Dynamic Chunk Fusion\n",
    "# ------------------------------------------------------------\n",
    "def fuse_adjacent_chunks(chunks, overlap_threshold=0.5, max_length=2000):\n",
    "    \"\"\"\n",
    "    Dynamically merges consecutive text chunks that show high overlap or continuity.\n",
    "    Produces longer, semantically coherent chunks for richer retrieval.\n",
    "    \"\"\"\n",
    "    fused_chunks = []\n",
    "    buffer = None\n",
    "\n",
    "    for c in chunks:\n",
    "        text = c.page_content.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if buffer is None:\n",
    "            buffer = c\n",
    "            continue\n",
    "\n",
    "        # Token-overlap ratio\n",
    "        buf_words = set(buffer.page_content.split())\n",
    "        new_words = set(text.split())\n",
    "        overlap = len(buf_words & new_words) / max(len(buf_words), 1)\n",
    "\n",
    "        # Merge if overlap high or current chunk still short\n",
    "        if overlap > overlap_threshold or len(buffer.page_content) < max_length:\n",
    "            buffer.page_content += \" \" + text\n",
    "        else:\n",
    "            fused_chunks.append(buffer)\n",
    "            buffer = c\n",
    "\n",
    "    if buffer:\n",
    "        fused_chunks.append(buffer)\n",
    "\n",
    "    print(f\"ðŸ§© Fused {len(chunks)} â†’ {len(fused_chunks)} chunks (context-optimized).\")\n",
    "    return fused_chunks\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Function: Build or Load LanceDB Index\n",
    "# ------------------------------------------------------------\n",
    "def build_or_load_lancedb_index(index_dir=INDEX_DIR, chunks=None):\n",
    "    \"\"\"\n",
    "    Builds or loads a LanceDB index from dynamically fused text chunks.\n",
    "    Generates longer, context-aware embeddings for improved coherence.\n",
    "    \"\"\"\n",
    "    index_dir.mkdir(parents=True, exist_ok=True)\n",
    "    db = lancedb.connect(str(index_dir))\n",
    "\n",
    "    # --- Load existing index ---\n",
    "    if \"documents\" in db.table_names():\n",
    "        print(\"ðŸ“¦ Existing LanceDB (Chunk-Fused) index found â€“ loading...\")\n",
    "        table = db.open_table(\"documents\")\n",
    "        vectorstore = LanceDB(connection=db, table=table, embedding=embeddings)\n",
    "        print(\"âœ… LanceDB (Chunk-Fused) index loaded successfully.\")\n",
    "        return vectorstore\n",
    "\n",
    "    if not chunks:\n",
    "        raise RuntimeError(\"âŒ No chunks provided. Please run Step 2 first.\")\n",
    "\n",
    "    # --- Apply fusion ---\n",
    "    fused_chunks = fuse_adjacent_chunks(chunks, overlap_threshold=0.5, max_length=2000)\n",
    "\n",
    "    print(f\"ðŸ§± Building LanceDB (Chunk-Fused) index on {DEVICE.upper()}...\")\n",
    "    print(f\"ðŸ“Š Total fused chunks to embed: {len(fused_chunks):,}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    vectorstore = LanceDB.from_documents(\n",
    "        tqdm(fused_chunks, desc=\"ðŸ”¢ Embedding fused text chunks\", ncols=100),\n",
    "        embedding=embeddings,\n",
    "        uri=str(index_dir),\n",
    "        table_name=\"documents\",\n",
    "    )\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"ðŸ’¾ Saved LanceDB (Chunk-Fused) index â†’ {index_dir}\")\n",
    "    print(f\"â±ï¸ Build completed in {duration/60:.2f} min ({duration:.1f} sec)\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3 Execution\n",
    "# ------------------------------------------------------------\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": DEVICE},\n",
    "    encode_kwargs={\"batch_size\": BATCH_SIZE}\n",
    ")\n",
    "\n",
    "vectorstore = build_or_load_lancedb_index(INDEX_DIR, chunks)\n",
    "\n",
    "# --- Configure retriever ---\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": TOP_K}\n",
    ")\n",
    "\n",
    "print(f\"âœ… Retriever ready (LanceDB + Chunk Fusion + {DEVICE.upper()}, top_k={TOP_K})\")\n",
    "print(\"ðŸ“ˆ Configuration Summary:\")\n",
    "print(f\"   â€¢ Embedding Model: {EMBED_MODEL}\")\n",
    "print(f\"   â€¢ Chunk Fusion: enabled (dynamic overlap > 0.5)\")\n",
    "print(f\"   â€¢ Vector DB: LanceDB\")\n",
    "print(f\"   â€¢ Top-k: {TOP_K}\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Device: {DEVICE.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ \n",
    "# STEP 3 â€” Speed & Scalability Benchmark (RAG-Version 9)\n",
    "# ============================================\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from langchain_community.vectorstores import LanceDB, FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import lancedb\n",
    "try:\n",
    "    from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection\n",
    "    MILVUS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MILVUS_AVAILABLE = False\n",
    "    print(\"âš ï¸ Milvus not installed â€” skipping that backend.\")\n",
    "\n",
    "# --- Parameters ---\n",
    "TOP_K = 8\n",
    "EMBED_MODEL = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "INDEX_DIR = Path(\"data/index/v9_benchmark\")\n",
    "RESULTS_PATH = Path(\"data/results/v9_benchmark_results.csv\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"ðŸ’» Using device: {DEVICE.upper()}\")\n",
    "\n",
    "# --- Adaptive batch size ---\n",
    "def auto_batch_size():\n",
    "    if DEVICE != \"cuda\": return 16\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    return 128 if vram_gb >= 22 else (64 if vram_gb >= 12 else 32)\n",
    "\n",
    "BATCH_SIZE = auto_batch_size()\n",
    "print(f\"ðŸ§  VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB | Batch Size = {BATCH_SIZE}\")\n",
    "\n",
    "# --- Initialize embedding model ---\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": DEVICE},\n",
    "    encode_kwargs={\"batch_size\": BATCH_SIZE}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: measure recall@k\n",
    "# ------------------------------------------------------------\n",
    "def recall_at_k(retriever, queries, gold_answers, k=TOP_K):\n",
    "    correct = 0\n",
    "    for q, gold in zip(queries, gold_answers):\n",
    "        results = retriever.get_relevant_documents(q)\n",
    "        texts = [r.page_content for r in results]\n",
    "        if any(g in \" \".join(texts) for g in gold.split()):\n",
    "            correct += 1\n",
    "    return correct / len(queries)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Benchmark LanceDB\n",
    "# ------------------------------------------------------------\n",
    "def benchmark_lancedb(chunks):\n",
    "    print(\"\\nðŸš€ Benchmarking LanceDB ...\")\n",
    "    db_path = INDEX_DIR / \"lancedb\"\n",
    "    db_path.mkdir(parents=True, exist_ok=True)\n",
    "    db = lancedb.connect(str(db_path))\n",
    "    start = time.time()\n",
    "    vectorstore = LanceDB.from_documents(\n",
    "        tqdm(chunks, desc=\"ðŸ”¹ Embedding (LanceDB)\", ncols=90),\n",
    "        embedding=embeddings,\n",
    "        uri=str(db_path),\n",
    "        table_name=\"documents\"\n",
    "    )\n",
    "    build_time = time.time() - start\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "    q_sample = [c.page_content[:200] for c in random.sample(chunks, min(20, len(chunks)))]\n",
    "    start = time.time()\n",
    "    for q in q_sample:\n",
    "        retriever.get_relevant_documents(q)\n",
    "    query_time = (time.time() - start) / len(q_sample)\n",
    "    mem = psutil.virtual_memory().used / 1e9\n",
    "    return {\"DB\":\"LanceDB\",\"Build_s\":build_time,\"Query_s\":query_time,\"Mem_GB\":mem}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Benchmark FAISS\n",
    "# ------------------------------------------------------------\n",
    "def benchmark_faiss(chunks):\n",
    "    print(\"\\nðŸš€ Benchmarking FAISS (HNSW) ...\")\n",
    "    start = time.time()\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    build_time = time.time() - start\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "    q_sample = [c.page_content[:200] for c in random.sample(chunks, min(20, len(chunks)))]\n",
    "    start = time.time()\n",
    "    for q in q_sample:\n",
    "        retriever.get_relevant_documents(q)\n",
    "    query_time = (time.time() - start) / len(q_sample)\n",
    "    mem = psutil.virtual_memory().used / 1e9\n",
    "    return {\"DB\":\"FAISS\",\"Build_s\":build_time,\"Query_s\":query_time,\"Mem_GB\":mem}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Benchmark Milvus (optional)\n",
    "# ------------------------------------------------------------\n",
    "def benchmark_milvus(chunks):\n",
    "    if not MILVUS_AVAILABLE:\n",
    "        return None\n",
    "    print(\"\\nðŸš€ Benchmarking Milvus ...\")\n",
    "    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "    if utility.has_collection(\"rag_v9\"): utility.drop_collection(\"rag_v9\")\n",
    "    schema = CollectionSchema([\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=len(embeddings.embed_query(\"test\"))),\n",
    "    ], \"RAG V9 Benchmark\")\n",
    "    collection = Collection(\"rag_v9\", schema)\n",
    "    start = time.time()\n",
    "    vecs = [embeddings.embed_query(c.page_content) for c in tqdm(chunks[:5000], desc=\"ðŸ”¹ Milvus embedding\")]\n",
    "    collection.insert([list(range(len(vecs))), vecs])\n",
    "    collection.create_index(\"embedding\", {\"index_type\":\"HNSW\",\"metric_type\":\"L2\"})\n",
    "    build_time = time.time() - start\n",
    "    collection.load()\n",
    "    q_sample = [c.page_content[:200] for c in random.sample(chunks, min(20, len(chunks)))]\n",
    "    start = time.time()\n",
    "    for q in q_sample:\n",
    "        qv = embeddings.embed_query(q)\n",
    "        collection.search([qv],\"embedding\",param={\"metric_type\":\"L2\",\"params\":{\"ef\":32}},limit=TOP_K)\n",
    "    query_time = (time.time() - start) / len(q_sample)\n",
    "    mem = psutil.virtual_memory().used / 1e9\n",
    "    return {\"DB\":\"Milvus\",\"Build_s\":build_time,\"Query_s\":query_time,\"Mem_GB\":mem}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Execute benchmark\n",
    "# ------------------------------------------------------------\n",
    "def run_benchmark(chunks):\n",
    "    results = []\n",
    "    results.append(benchmark_lancedb(chunks))\n",
    "    results.append(benchmark_faiss(chunks))\n",
    "    if MILVUS_AVAILABLE:\n",
    "        milvus_res = benchmark_milvus(chunks)\n",
    "        if milvus_res: results.append(milvus_res)\n",
    "    print(\"\\nðŸ“Š Benchmark Results:\")\n",
    "    for r in results:\n",
    "        print(f\" {r['DB']:8s} | Build {r['Build_s']/60:.2f} min | Query {r['Query_s']*1000:.2f} ms | Mem {r['Mem_GB']:.1f} GB\")\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    RESULTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(RESULTS_PATH, index=False)\n",
    "    print(f\"ðŸ’¾ Saved results â†’ {RESULTS_PATH}\")\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run\n",
    "# ------------------------------------------------------------\n",
    "df_results = run_benchmark(chunks)\n",
    "print(\"\\nâœ… RAG-Version 9 Speed & Scalability Benchmark completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

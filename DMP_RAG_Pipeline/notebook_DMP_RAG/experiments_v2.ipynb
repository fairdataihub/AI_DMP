{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0f6b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 10/08/2025\n"
     ]
    }
   ],
   "source": [
    "print (\"Start 10/08/2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb52b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ STEP 1 ready\n",
      "ROOT_DIR   : c:\\Users\\Nahid\\DMP-RAG\n",
      "DATA_PDFS  : c:\\Users\\Nahid\\DMP-RAG\\data\\NIH_95\n",
      "INDEX_DIR  : c:\\Users\\Nahid\\DMP-RAG\\data\\faiss_index\n",
      "EXCEL_PATH : c:\\Users\\Nahid\\DMP-RAG\\data\\inputs\\inputs.xlsx\n",
      "TEMPLATE_MD: c:\\Users\\Nahid\\DMP-RAG\\data\\inputs\\dmp-template.md\n",
      "OUTPUT_MD  : c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\n",
      "OUTPUT_DOCX: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\n",
      "EMBED_MODEL: sentence-transformers/all-MiniLM-L6-v2 | LLM_MODEL: llama3.3 | TOP_K: 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1 — Imports, Config, and Helpers\n",
    "# ============================================\n",
    "import os, re, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pypandoc  # for Markdown → DOCX\n",
    "\n",
    "# --- LangChain Core ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------- Paths (works in notebook or script) ----------\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parents[1]  # when running a .py script\n",
    "except NameError:\n",
    "    ROOT_DIR = Path.cwd().parent                     # when running inside Jupyter\n",
    "\n",
    "# --- Data folders ---\n",
    "DATA_PDFS   = ROOT_DIR / \"data\" / \"NIH_95\"\n",
    "INDEX_DIR   = ROOT_DIR / \"data\" / \"faiss_index\"\n",
    "EXCEL_PATH  = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "TEMPLATE_MD = ROOT_DIR / \"data\" / \"inputs\" / \"dmp-template.md\"\n",
    "\n",
    "# --- Output folders ---\n",
    "OUTPUT_MD   = ROOT_DIR / \"data\" / \"outputs\" / \"markdown\"\n",
    "OUTPUT_DOCX = ROOT_DIR / \"data\" / \"outputs\" / \"docx\"\n",
    "\n",
    "# --- Models / parameters ---\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL   = \"llama3.3\"\n",
    "TOP_K       = 6\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def create_folder(folderpath):\n",
    "    Path(folderpath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_md(folderpath, filename, text):\n",
    "    create_folder(folderpath)\n",
    "    (Path(folderpath) / filename).write_text(text, encoding=\"utf-8\")\n",
    "    print(\"💾 Saved:\", Path(folderpath) / filename)\n",
    "\n",
    "def md_to_docs(md_filepath, docx_folderpath, docx_filename):\n",
    "    create_folder(docx_folderpath)\n",
    "    pypandoc.convert_file(\n",
    "        str(md_filepath), \"docx\",\n",
    "        outputfile=str(Path(docx_folderpath) / docx_filename)\n",
    "    )\n",
    "    print(\"📄 Converted:\", Path(docx_folderpath) / docx_filename)\n",
    "\n",
    "def clean_filename(name: str) -> str:\n",
    "    \"\"\"Remove illegal characters from filenames (Windows-safe).\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", str(name)).strip()\n",
    "\n",
    "# ---------- Ensure required folders exist ----------\n",
    "for p in [DATA_PDFS, INDEX_DIR, OUTPUT_MD, OUTPUT_DOCX]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Sanity print ----------\n",
    "print(\"✅ STEP 1 ready\")\n",
    "print(f\"ROOT_DIR   : {ROOT_DIR}\")\n",
    "print(f\"DATA_PDFS  : {DATA_PDFS}\")\n",
    "print(f\"INDEX_DIR  : {INDEX_DIR}\")\n",
    "print(f\"EXCEL_PATH : {EXCEL_PATH}\")\n",
    "print(f\"TEMPLATE_MD: {TEMPLATE_MD}\")\n",
    "print(f\"OUTPUT_MD  : {OUTPUT_MD}\")\n",
    "print(f\"OUTPUT_DOCX: {OUTPUT_DOCX}\")\n",
    "print(f\"EMBED_MODEL: {EMBED_MODEL} | LLM_MODEL: {LLM_MODEL} | TOP_K: {TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687935b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📥 Loading PDFs: 100%|██████████| 94/94 [00:17<00:00,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 520 pages from 94 PDFs.\n",
      "✅ Created 1782 chunks from 520 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2 — Load PDFs and Split into Text Chunks\n",
    "# ============================================\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_pdfs_from_folder(folder: Path):\n",
    "    \"\"\"Load all PDF files from a folder into LangChain Document objects.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"❌ Folder not found: {folder}\")\n",
    "    pdf_files = sorted(folder.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"⚠️ No PDF files found in {folder}\")\n",
    "\n",
    "    docs = []\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"📥 Loading PDFs\"):\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_path))\n",
    "            docs.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipped {pdf_path.name}: {e}\")\n",
    "\n",
    "    print(f\"✅ Loaded {len(docs)} pages from {len(pdf_files)} PDFs.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_into_chunks(docs, chunk_size=800, chunk_overlap=120):\n",
    "    \"\"\"Split PDF text into overlapping chunks for embedding/indexing.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"✅ Created {len(chunks)} chunks from {len(docs)} pages.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Run quick test ---\n",
    "raw_docs = load_pdfs_from_folder(DATA_PDFS)\n",
    "chunks = split_into_chunks(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0f00cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nahid\\AppData\\Local\\Temp\\ipykernel_24824\\2321972389.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Existing FAISS index found. Loading from disk...\n",
      "✅ FAISS index loaded successfully.\n",
      "✅ Retriever ready (top_k=6)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3 — Build or Load FAISS Index\n",
    "# ============================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import time\n",
    "\n",
    "# --- Initialize embedding model ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "def build_or_load_faiss_index(index_dir=INDEX_DIR, chunks=None):\n",
    "    \"\"\"\n",
    "    Builds a new FAISS index from text chunks if none exists,\n",
    "    otherwise loads the saved one from disk.\n",
    "    \"\"\"\n",
    "    faiss_path = index_dir / \"index.faiss\"\n",
    "    pkl_path   = index_dir / \"index.pkl\"\n",
    "\n",
    "    # --- If index exists, load it ---\n",
    "    if faiss_path.exists() and pkl_path.exists():\n",
    "        print(\"📦 Existing FAISS index found. Loading from disk...\")\n",
    "        vectorstore = FAISS.load_local(\n",
    "            str(index_dir),\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"✅ FAISS index loaded successfully.\")\n",
    "        return vectorstore\n",
    "\n",
    "    # --- Otherwise, build new index ---\n",
    "    if chunks is None or len(chunks) == 0:\n",
    "        raise RuntimeError(\"❌ No chunks provided. Please run Step 2 first to load and split PDFs.\")\n",
    "\n",
    "    print(\"🧱 Building new FAISS index...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        tqdm(chunks, desc=\"🔢 Embedding text chunks\"),\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    # --- Save the index ---\n",
    "    vectorstore.save_local(str(index_dir))\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(f\"💾 Saved new FAISS index to {index_dir}\")\n",
    "    print(f\"⏱️ Build completed in {duration/60:.2f} minutes ({duration:.1f} seconds)\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# --- Execute step ---\n",
    "vectorstore = build_or_load_faiss_index(INDEX_DIR, chunks)\n",
    "retriever   = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "print(f\"✅ Retriever ready (top_k={TOP_K})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d9b2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel loaded successfully: 26 rows\n",
      "✅ DMP Markdown template loaded.\n",
      "🔗 RAG chain initialized with model: llama3.3\n",
      "✅ RAG chain ready for generation.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🧩 STEP 4 — Load Excel, Template, and Build RAG Chain (Fixed)\n",
    "# ============================================\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load Excel file ---\n",
    "if not EXCEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"❌ Excel file not found: {EXCEL_PATH}\")\n",
    "\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "print(f\"✅ Excel loaded successfully: {len(df)} rows\")\n",
    "\n",
    "# --- Load Markdown Template ---\n",
    "if not TEMPLATE_MD.exists():\n",
    "    raise FileNotFoundError(f\"❌ Template file not found: {TEMPLATE_MD}\")\n",
    "\n",
    "dmp_template_text = TEMPLATE_MD.read_text(encoding=\"utf-8\")\n",
    "print(\"✅ DMP Markdown template loaded.\")\n",
    "\n",
    "\n",
    "# --- Build RAG chain ---\n",
    "def build_rag_chain(retriever, llm_model=LLM_MODEL):\n",
    "    \"\"\"\n",
    "    Build a flexible RAG pipeline that retrieves context\n",
    "    and generates a context-grounded NIH DMP section.\n",
    "    \"\"\"\n",
    "    llm = Ollama(model=llm_model)\n",
    "\n",
    "    prompt_template = \"\"\"You are an expert biomedical data steward and grant writer.\n",
    "Create a high-quality NIH Data Management and Sharing Plan (DMSP)\n",
    "based on the retrieved NIH context and the user's query.\n",
    "\n",
    "----\n",
    "Context from NIH Repository:\n",
    "{context}\n",
    "\n",
    "----\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Use the context above and follow the NIH template structure. Write fluently and cohesively.\n",
    "\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    def format_docs(docs):\n",
    "        \"\"\"Format retrieved documents into clean text.\"\"\"\n",
    "        if not docs:\n",
    "            return \"\"\n",
    "        formatted = []\n",
    "        for d in docs:\n",
    "            page = d.metadata.get(\"page\", \"\")\n",
    "            title = d.metadata.get(\"source\", \"\")\n",
    "            formatted.append(f\"[Page {page}] {title}\\n{d.page_content.strip()}\")\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "\n",
    "    print(f\"🔗 RAG chain initialized with model: {llm_model}\")\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# --- Initialize the RAG chain ---\n",
    "rag_chain = build_rag_chain(retriever)\n",
    "print(\"✅ RAG chain ready for generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dd1758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded input Excel — 26 rows\n",
      "✅ Loaded NIH DMP Markdown template from: c:\\Users\\Nahid\\DMP-RAG\\data\\inputs\\dmp-template.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧩 Generating DMP for: Clinical and MRI data from human research participants\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:   4%|▍         | 1/26 [01:23<34:47, 83.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Clinical and MRI data from human research participants.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Clinical and MRI data from human research participants.docx\n",
      "\n",
      "🧩 Generating DMP for: Genomic data from human research participants\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:   8%|▊         | 2/26 [02:52<34:36, 86.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Genomic data from human research participants.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Genomic data from human research participants.docx\n",
      "\n",
      "🧩 Generating DMP for: Genomic data from a non-human source\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  12%|█▏        | 3/26 [04:18<33:06, 86.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Genomic data from a non-human source.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Genomic data from a non-human source.docx\n",
      "\n",
      "🧩 Generating DMP for: Secondary data analysis\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  15%|█▌        | 4/26 [05:48<32:15, 88.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Secondary data analysis.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Secondary data analysis.docx\n",
      "\n",
      "🧩 Generating DMP for: Human clinical and genomics data\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  19%|█▉        | 5/26 [07:14<30:27, 87.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Human clinical and genomics data.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Human clinical and genomics data.docx\n",
      "\n",
      "🧩 Generating DMP for: Gene expression analysis data from non-human model organism (zebrafish)\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  23%|██▎       | 6/26 [08:41<29:00, 87.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Gene expression analysis data from non-human model organism (zebrafish).md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Gene expression analysis data from non-human model organism (zebrafish).docx\n",
      "\n",
      "🧩 Generating DMP for: Human survey data\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  27%|██▋       | 7/26 [10:08<27:34, 87.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Human survey data.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Human survey data.docx\n",
      "\n",
      "🧩 Generating DMP for: Clinical Data from Human Research Participants\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  31%|███       | 8/26 [11:26<25:13, 84.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Clinical Data from Human Research Participants.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Clinical Data from Human Research Participants.docx\n",
      "\n",
      "🧩 Generating DMP for: Human genomic data\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  35%|███▍      | 9/26 [13:15<26:02, 91.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Human genomic data.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Human genomic data.docx\n",
      "\n",
      "🧩 Generating DMP for: Technology development\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  38%|███▊      | 10/26 [14:49<24:40, 92.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Technology development.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Technology development.docx\n",
      "\n",
      "🧩 Generating DMP for: Basic Research from a Non-Human Source Example\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  42%|████▏     | 11/26 [16:26<23:28, 93.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Basic Research from a Non-Human Source Example.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Basic Research from a Non-Human Source Example.docx\n",
      "\n",
      "🧩 Generating DMP for: Secondary Data Analysis Example\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  46%|████▌     | 12/26 [17:45<20:51, 89.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Secondary Data Analysis Example.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Secondary Data Analysis Example.docx\n",
      "\n",
      "🧩 Generating DMP for: Survey and Interview Example\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  50%|█████     | 13/26 [19:06<18:50, 86.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Survey and Interview Example.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Survey and Interview Example.docx\n",
      "\n",
      "🧩 Generating DMP for: Human Clinical Trial Data\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  54%|█████▍    | 14/26 [20:23<16:47, 83.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Human Clinical Trial Data.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Human Clinical Trial Data.docx\n",
      "\n",
      "🧩 Generating DMP for: Clinical data from human research participants-NIA\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  58%|█████▊    | 15/26 [21:40<14:59, 81.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Clinical data from human research participants-NIA.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Clinical data from human research participants-NIA.docx\n",
      "\n",
      "🧩 Generating DMP for: Survey, interview, and biological data (tiered access)\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  62%|██████▏   | 16/26 [22:44<12:45, 76.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Survey, interview, and biological data (tiered access).md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Survey, interview, and biological data (tiered access).docx\n",
      "\n",
      "🧩 Generating DMP for: Non-human data (primates)\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  65%|██████▌   | 17/26 [23:48<10:54, 72.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Non-human data (primates).md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Non-human data (primates).docx\n",
      "\n",
      "🧩 Generating DMP for: Secondary data analysis-NIA\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  69%|██████▉   | 18/26 [25:08<10:00, 75.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Secondary data analysis-NIA.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Secondary data analysis-NIA.docx\n",
      "\n",
      "🧩 Generating DMP for: Survey and interview data-NIA\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  73%|███████▎  | 19/26 [26:16<08:30, 72.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Survey and interview data-NIA.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Survey and interview data-NIA.docx\n",
      "\n",
      "🧩 Generating DMP for: Human clinical and genomic data-NIA\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  77%|███████▋  | 20/26 [28:10<08:30, 85.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Human clinical and genomic data-NIA.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Human clinical and genomic data-NIA.docx\n",
      "\n",
      "🧩 Generating DMP for: Non-human data (rodents)-NIA\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  81%|████████  | 21/26 [29:36<07:07, 85.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Non-human data (rodents)-NIA.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Non-human data (rodents)-NIA.docx\n",
      "\n",
      "🧩 Generating DMP for: Clinical data (human biospecimens)\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  85%|████████▍ | 22/26 [30:59<05:38, 84.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Clinical data (human biospecimens).md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Clinical data (human biospecimens).docx\n",
      "\n",
      "🧩 Generating DMP for: Drug discovery including intellectual property\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  88%|████████▊ | 23/26 [32:21<04:12, 84.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Drug discovery including intellectual property.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Drug discovery including intellectual property.docx\n",
      "\n",
      "🧩 Generating DMP for: HeLa Cell Whole Genome Sequence (DNA or RNA)\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  92%|█████████▏| 24/26 [33:37<02:42, 81.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\HeLa Cell Whole Genome Sequence (DNA or RNA).md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\HeLa Cell Whole Genome Sequence (DNA or RNA).docx\n",
      "\n",
      "🧩 Generating DMP for: Secondary Data Analysis on Data from Human Subjects-NIA\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs:  96%|█████████▌| 25/26 [34:46<01:17, 77.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Secondary Data Analysis on Data from Human Subjects-NIA.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Secondary Data Analysis on Data from Human Subjects-NIA.docx\n",
      "\n",
      "🧩 Generating DMP for: Analysis of social media posts\n",
      "🔎 Retrieved 6 context chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Generating NIH DMPs: 100%|██████████| 26/26 [36:05<00:00, 83.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\\Analysis of social media posts.md\n",
      "📄 Converted: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\docx\\Analysis of social media posts.docx\n",
      "\n",
      "✅ All NIH DMPs generated successfully — titles preserved exactly as in Excel!\n",
      "📊 CSV log saved to: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\rag_generated_dmp_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🧩 STEP 5 — RAG-Based DMP Generation Using Titles\n",
    "# ============================================\n",
    "import re, pandas as pd, pypandoc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Paths ----------\n",
    "EXCEL_PATH = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "OUTPUT_LOG = ROOT_DIR / \"data\" / \"outputs\" / \"rag_generated_dmp_log.csv\"\n",
    "OUTPUT_MD.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DOCX.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load Excel ----------\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "print(f\"✅ Loaded input Excel — {len(df)} rows\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# ---------- Verify template ----------\n",
    "if not TEMPLATE_MD.exists():\n",
    "    raise FileNotFoundError(f\"❌ Template not found: {TEMPLATE_MD}\")\n",
    "dmp_template_text = TEMPLATE_MD.read_text(encoding=\"utf-8\")\n",
    "print(f\"✅ Loaded NIH DMP Markdown template from: {TEMPLATE_MD}\")\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"Replace illegal filename characters but preserve readable title.\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name.strip())\n",
    "\n",
    "def create_folder(folderpath: Path):\n",
    "    folderpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_md(folderpath: Path, filename: str, response: str):\n",
    "    create_folder(folderpath)\n",
    "    filepath = folderpath / filename\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response)\n",
    "    print(f\"💾 Saved: {filepath}\")\n",
    "\n",
    "def md_to_docx(md_filepath: Path, docx_folder: Path, docx_filename: str):\n",
    "    create_folder(docx_folder)\n",
    "    docx_path = docx_folder / docx_filename\n",
    "    pypandoc.convert_file(str(md_filepath), \"docx\", outputfile=str(docx_path))\n",
    "    print(f\"📄 Converted: {docx_path}\")\n",
    "\n",
    "# ---------- Main Generation ----------\n",
    "records = []\n",
    "TOP_K = 6  # retrieved context chunks\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"🧠 Generating NIH DMPs\"):\n",
    "    title = str(row[\"title\"]).strip()\n",
    "    print(f\"\\n🧩 Generating DMP for: {title}\")\n",
    "\n",
    "    # 1️⃣ Build query from Excel elements\n",
    "    element_texts = []\n",
    "    for col in [c for c in df.columns if c.startswith(\"element\")]:\n",
    "        val = str(row[col]).strip()\n",
    "        if val:\n",
    "            element_texts.append(f\"{col.upper()}: {val}\")\n",
    "    query_data = \"\\n\".join(element_texts)\n",
    "\n",
    "    query = (\n",
    "        f\"You are an expert biomedical data steward and grant writer. \"\n",
    "        f\"Create a complete NIH Data Management and Sharing Plan (DMSP) for the project titled '{title}'. \"\n",
    "        f\"Use retrieved context from the NIH corpus to fill in all template sections accurately.\\n\\n\"\n",
    "        f\"Here is background information from the proposal:\\n{query_data}\\n\"\n",
    "    )\n",
    "\n",
    "    # 2️⃣ Retrieve context from FAISS\n",
    "    try:\n",
    "        retrieved_docs = retriever.get_relevant_documents(query)\n",
    "        context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs[:TOP_K])\n",
    "        print(f\"🔎 Retrieved {len(retrieved_docs)} context chunks.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Retrieval failed for {title}: {e}\")\n",
    "        context_text = \"\"\n",
    "\n",
    "    # 3️⃣ Combine context, query, and template\n",
    "    full_prompt = f\"\"\"\n",
    "You are an expert biomedical data steward and grant writer.\n",
    "Use the retrieved NIH context and the provided template to generate a complete Data Management and Sharing Plan.\n",
    "\n",
    "----\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "----\n",
    "Project Query:\n",
    "{query}\n",
    "\n",
    "Use the following NIH DMSP Markdown template. Do not alter section titles:\n",
    "{dmp_template_text}\n",
    "\"\"\"\n",
    "\n",
    "    # 4️⃣ Run through RAG model\n",
    "    try:\n",
    "        response = rag_chain.invoke(full_prompt)\n",
    "\n",
    "        # 5️⃣ Save using SAME TITLE as in Excel\n",
    "        safe_title = sanitize_filename(title)\n",
    "        md_filename = f\"{safe_title}.md\"\n",
    "        docx_filename = f\"{safe_title}.docx\"\n",
    "        md_path = OUTPUT_MD / md_filename\n",
    "\n",
    "        save_md(OUTPUT_MD, md_filename, response)\n",
    "        md_to_docx(md_path, OUTPUT_DOCX, docx_filename)\n",
    "\n",
    "        # 6️⃣ Log summary\n",
    "        records.append({\n",
    "            \"Title\": title,\n",
    "            \"Query\": query,\n",
    "            \"Retrieved_Context\": context_text[:1000],\n",
    "            \"Generated_DMP_Preview\": response[:1000],\n",
    "            \"Error\": \"\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating DMP for {title}: {e}\")\n",
    "        records.append({\n",
    "            \"Title\": title,\n",
    "            \"Query\": query,\n",
    "            \"Retrieved_Context\": context_text[:1000],\n",
    "            \"Generated_DMP_Preview\": \"\",\n",
    "            \"Error\": str(e)\n",
    "        })\n",
    "\n",
    "# ---------- Save Log ----------\n",
    "pd.DataFrame(records).to_csv(OUTPUT_LOG, index=False, encoding=\"utf-8\")\n",
    "print(\"\\n✅ All NIH DMPs generated successfully — titles preserved exactly as in Excel!\")\n",
    "print(f\"📊 CSV log saved to: {OUTPUT_LOG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db58c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 ROOT_DIR set to: c:\\Users\\Nahid\\DMP-RAG\n",
      "📗 Gold PDF folder: c:\\Users\\Nahid\\DMP-RAG\\data\\inputs\\gold_dmps\n",
      "📘 Generated Markdown folder: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\n",
      "🚀 Loading models...\n",
      "✅ Models ready.\n",
      "📊 Found 26 generated DMPs and 26 gold PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:   4%|▍         | 1/26 [00:00<00:08,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Analysis of social media posts.md ↔ 26-Analysis of social media posts-NCI.pdf (score=0.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:   8%|▊         | 2/26 [00:00<00:08,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Basic Research from a Non-Human Source Example.md ↔ 11-Basic Research from a Non-Human Source Example-NIDDK.pdf (score=0.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  12%|█▏        | 3/26 [00:01<00:09,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Clinical and MRI data from human research participants.md ↔ 1-Clinical andor MRI data from human research participants-NIMH.pdf (score=0.92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  15%|█▌        | 4/26 [00:01<00:09,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Clinical data (human biospecimens).md ↔ 22-Clinical data (human biospecimens)-NIA.pdf (score=0.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  19%|█▉        | 5/26 [00:01<00:07,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Clinical data from human research participants-NIA.md ↔ 15-Clinical data from human research participants-NIA.pdf (score=0.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  23%|██▎       | 6/26 [00:02<00:06,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Clinical Data from Human Research Participants.md ↔ 15-Clinical data from human research participants-NIA.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  27%|██▋       | 7/26 [00:02<00:06,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Drug discovery including intellectual property.md ↔ 23-Drug discovery including intellectual property-NIA.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  31%|███       | 8/26 [00:02<00:06,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Gene expression analysis data from non-human model organism (zebrafish).md ↔ 8-Gene expression analysis data from non-human model organism (zebrafish)-NICHD.pdf (score=0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  35%|███▍      | 9/26 [00:03<00:06,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Genomic data from a non-human source.md ↔ 3-Genomic data from a non-human source-NIMH.pdf (score=0.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  38%|███▊      | 10/26 [00:03<00:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Genomic data from human research participants.md ↔ 2-Genomic data from human research participants-NIMH.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  42%|████▏     | 11/26 [00:04<00:05,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched HeLa Cell Whole Genome Sequence (DNA or RNA).md ↔ 24-HeLa Cell Whole Genome Sequence (DNA or RNA)-OD, NHGRI.pdf (score=0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  46%|████▌     | 12/26 [00:04<00:06,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Human clinical and genomic data-NIA.md ↔ 20-Human clinical and genomic data-NIA.pdf (score=0.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  50%|█████     | 13/26 [00:05<00:05,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Human clinical and genomics data.md ↔ 7-Human clinical and genomics data-NICHD.pdf (score=0.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  54%|█████▍    | 14/26 [00:05<00:05,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Human Clinical Trial Data.md ↔ 14-Human Clinical Trial Data-NICHD.pdf (score=0.85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  58%|█████▊    | 15/26 [00:06<00:05,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Human genomic data.md ↔ 5-Human genomic data-NHGRI.pdf (score=0.82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  62%|██████▏   | 16/26 [00:06<00:04,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Human survey data.md ↔ 9-Human survey data-NICHD.pdf (score=0.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  65%|██████▌   | 17/26 [00:07<00:03,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Non-human data (primates).md ↔ 17-Non-human data (primates)-NIA.pdf (score=0.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  69%|██████▉   | 18/26 [00:07<00:03,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Non-human data (rodents)-NIA.md ↔ 21-Non-human data (rodents)-NIA.pdf (score=0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  73%|███████▎  | 19/26 [00:07<00:02,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Secondary Data Analysis Example.md ↔ 12-Secondary Data Analysis Example-NIDDK.pdf (score=0.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  77%|███████▋  | 20/26 [00:08<00:02,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Secondary Data Analysis on Data from Human Subjects-NIA.md ↔ 25-Secondary Data Analysis on Data from Human Subjects-NIA.pdf (score=0.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  81%|████████  | 21/26 [00:08<00:01,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Secondary data analysis-NIA.md ↔ 18-Secondary data analysis-NIA.pdf (score=0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  85%|████████▍ | 22/26 [00:08<00:01,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Secondary data analysis.md ↔ 18-Secondary data analysis-NIA.pdf (score=0.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  88%|████████▊ | 23/26 [00:09<00:01,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Survey and interview data-NIA.md ↔ 19-Survey and interview data-NIA.pdf (score=0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  92%|█████████▏| 24/26 [00:09<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Survey and Interview Example.md ↔ 13-Survey and Interview Example-NHGRI.pdf (score=0.86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs:  96%|█████████▌| 25/26 [00:09<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Survey, interview, and biological data (tiered access).md ↔ 16-Survey, interview, and biological data (tiered access)-NIA.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔎 Matching & Comparing DMPs: 100%|██████████| 26/26 [00:10<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matched Technology development.md ↔ 6-Technology development-NHGRI.pdf (score=0.85)\n",
      "\n",
      "✅ Markdown–PDF (fuzzy) similarity results saved to: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\evaluation_results\\full_dmp_pdf_comparison_fuzzy.csv\n",
      "🧾 Total matched DMP pairs: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🧩 STEP 7 — Full DMP Comparison: Markdown (Generated) vs PDF (Gold, Fuzzy Matching)\n",
    "# ============================================\n",
    "import os, re\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# --- Define ROOT_DIR dynamically ---\n",
    "ROOT_DIR = Path.cwd()\n",
    "while ROOT_DIR.name != \"DMP-RAG\" and ROOT_DIR.parent != ROOT_DIR:\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "print(f\"📂 ROOT_DIR set to: {ROOT_DIR}\")\n",
    "\n",
    "# --- Paths ---\n",
    "GOLD_DIR      = ROOT_DIR / \"data\" / \"inputs\" / \"gold_dmps\"      # PDF gold-standard DMPs\n",
    "GENERATED_DIR = ROOT_DIR / \"data\" / \"outputs\" / \"markdown\"      # Generated DMPs\n",
    "EVAL_DIR      = ROOT_DIR / \"data\" / \"outputs\" / \"evaluation_results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📗 Gold PDF folder: {GOLD_DIR}\")\n",
    "print(f\"📘 Generated Markdown folder: {GENERATED_DIR}\")\n",
    "\n",
    "# --- Models ---\n",
    "print(\"🚀 Loading models...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "print(\"✅ Models ready.\")\n",
    "\n",
    "# --- Helper functions ---\n",
    "def normalize_name(name: str) -> str:\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name.strip()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Remove markdown or formatting artifacts.\"\"\"\n",
    "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"#+\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\*\\*|\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract readable text from PDF using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text(\"text\") + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {pdf_path.name}: {e}\")\n",
    "    return clean_text(text)\n",
    "\n",
    "def chunk_text(text, size=300):\n",
    "    \"\"\"Split long text into 300-word chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+size]) for i in range(0, len(words), size)]\n",
    "\n",
    "def compare_chunked(gold_text, gen_text, model):\n",
    "    \"\"\"Chunked SBERT + ROUGE similarity between two long texts.\"\"\"\n",
    "    gold_chunks = chunk_text(gold_text)\n",
    "    gen_chunks = chunk_text(gen_text)\n",
    "\n",
    "    sbert_scores, rouge_scores = [], []\n",
    "    for g in gold_chunks:\n",
    "        emb_g = model.encode(g, convert_to_tensor=True)\n",
    "        chunk_sims = []\n",
    "        for gen in gen_chunks:\n",
    "            emb_gen = model.encode(gen, convert_to_tensor=True)\n",
    "            chunk_sims.append(util.cos_sim(emb_g, emb_gen).item())\n",
    "        sbert_scores.append(max(chunk_sims))  # best match per gold chunk\n",
    "\n",
    "        rouge_chunk_scores = [rouge.score(g, gen)[\"rougeL\"].recall for gen in gen_chunks]\n",
    "        rouge_scores.append(max(rouge_chunk_scores))\n",
    "\n",
    "    return np.mean(sbert_scores), np.mean(rouge_scores)\n",
    "\n",
    "def best_fuzzy_match(target, gold_names, threshold=0.6):\n",
    "    \"\"\"Find best matching name among gold files using fuzzy ratio.\"\"\"\n",
    "    best_match, best_score = None, 0\n",
    "    for g in gold_names:\n",
    "        score = SequenceMatcher(None, target, g).ratio()\n",
    "        if score > best_score:\n",
    "            best_match, best_score = g, score\n",
    "    return (best_match, best_score) if best_score >= threshold else (None, best_score)\n",
    "\n",
    "# --- Collect gold PDFs and generated MDs ---\n",
    "gold_files = {normalize_name(f.stem): f for f in GOLD_DIR.glob(\"*.pdf\")}\n",
    "gen_files  = {normalize_name(f.stem): f for f in GENERATED_DIR.glob(\"*.md\")}\n",
    "print(f\"📊 Found {len(gen_files)} generated DMPs and {len(gold_files)} gold PDFs.\")\n",
    "\n",
    "# --- Compare all matching files ---\n",
    "results = []\n",
    "for name, gen_path in tqdm(gen_files.items(), desc=\"🔎 Matching & Comparing DMPs\"):\n",
    "    best_match, score = best_fuzzy_match(name, list(gold_files.keys()))\n",
    "    if not best_match:\n",
    "        print(f\"⚠️ No gold match for: {gen_path.name}\")\n",
    "        continue\n",
    "\n",
    "    gold_path = gold_files[best_match]\n",
    "    gold_text = extract_text_from_pdf(gold_path)\n",
    "    gen_text  = clean_text(gen_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    if not gold_text.strip() or not gen_text.strip():\n",
    "        print(f\"⚠️ Skipping empty file: {name}\")\n",
    "        continue\n",
    "\n",
    "    sbert_sim, rouge_l = compare_chunked(gold_text, gen_text, sbert)\n",
    "    results.append({\n",
    "        \"Generated_File\": gen_path.name,\n",
    "        \"Matched_Gold_PDF\": gold_path.name,\n",
    "        \"Match_Score\": round(score, 3),\n",
    "        \"SBERT_Similarity\": round(sbert_sim, 4),\n",
    "        \"ROUGE_L_Recall\": round(rouge_l, 4),\n",
    "    })\n",
    "    print(f\"✅ Matched {gen_path.name} ↔ {gold_path.name} (score={score:.2f})\")\n",
    "\n",
    "# --- Save results ---\n",
    "df_results = pd.DataFrame(results)\n",
    "out_path = EVAL_DIR / \"full_dmp_pdf_comparison_fuzzy.csv\"\n",
    "df_results.to_csv(out_path, index=False)\n",
    "print(f\"\\n✅ Markdown–PDF (fuzzy) similarity results saved to: {out_path}\")\n",
    "print(f\"🧾 Total matched DMP pairs: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c307ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 ROOT_DIR set to: c:\\Users\\Nahid\\DMP-RAG\n",
      "📗 Gold Excel: c:\\Users\\Nahid\\DMP-RAG\\data\\inputs\\inputs.xlsx\n",
      "📘 Generated MD folder: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\markdown\n",
      "✅ Loaded 26 gold projects.\n",
      "🚀 Loading evaluation models...\n",
      "✅ Models ready.\n",
      "🔍 Found 26 generated Markdown files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📊 Comparing element-level: 100%|██████████| 26/26 [00:55<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Element-level similarity saved to: c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\evaluation_results\\element_similarity_exact_titles.csv\n",
      "🧾 Total element–section best matches: 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🧩 STEP 7 — Element-Level Comparison with NIH Gold Standard (Exact Title Match)\n",
    "# ============================================\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# --- Paths ---\n",
    "# --- Define ROOT_DIR dynamically (project root) ---\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically find the root (up to 'DMP-RAG')\n",
    "ROOT_DIR = Path.cwd()\n",
    "while ROOT_DIR.name != \"DMP-RAG\" and ROOT_DIR.parent != ROOT_DIR:\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "\n",
    "print(f\"📂 ROOT_DIR set to: {ROOT_DIR}\")\n",
    "GOLD_PATH      = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "GENERATED_DIR  = ROOT_DIR / \"data\" / \"outputs\" / \"markdown\"\n",
    "EVAL_DIR       = ROOT_DIR / \"data\" / \"outputs\" / \"evaluation_results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📗 Gold Excel: {GOLD_PATH}\")\n",
    "print(f\"📘 Generated MD folder: {GENERATED_DIR}\")\n",
    "\n",
    "# --- Load gold reference (Excel) ---\n",
    "df_gold = pd.read_excel(GOLD_PATH)\n",
    "df_gold.columns = df_gold.columns.str.strip().str.lower()\n",
    "df_gold = df_gold.fillna(\"\").astype(str)\n",
    "\n",
    "def normalize_title(name: str) -> str:\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name.strip()\n",
    "\n",
    "df_gold[\"title_norm\"] = df_gold[\"title\"].apply(normalize_title)\n",
    "\n",
    "gold_elements = [\n",
    "    \"element_1a\",\"element_1b\",\"element_1c\",\n",
    "    \"element_2\",\"element_3\",\n",
    "    \"element_4a\",\"element_4b\",\"element_4c\",\n",
    "    \"element_5a\",\"element_5b\",\"element_5c\",\n",
    "    \"element_6\"\n",
    "]\n",
    "print(f\"✅ Loaded {len(df_gold)} gold projects.\")\n",
    "\n",
    "# --- Models ---\n",
    "print(\"🚀 Loading evaluation models...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "print(\"✅ Models ready.\")\n",
    "\n",
    "# --- Markdown parsing helpers ---\n",
    "def is_title(line: str) -> bool:\n",
    "    s = line.strip()\n",
    "    # Accept markdown headers (#, ##, ...) OR numbered bold section titles like \"1. **Data Types**\"\n",
    "    return s.startswith(\"#\") or bool(re.match(r\"^\\s*\\d*\\.?\\s*\\*\\*.*\\*\\*\\s*$\", s))\n",
    "\n",
    "def extract_sections(md_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract {Section Title, Generated Content} pairs from a Markdown file.\n",
    "    Also strips any <think>...</think> blocks if present.\n",
    "    \"\"\"\n",
    "    text = md_path.read_text(encoding=\"utf-8\")\n",
    "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    entries, current_title, buf = [], None, []\n",
    "\n",
    "    for ln in lines:\n",
    "        if is_title(ln):\n",
    "            if current_title and any(x.strip() for x in buf):\n",
    "                entries.append({\n",
    "                    \"Section Title\": current_title.strip(),\n",
    "                    \"Generated Content\": \"\\n\".join(buf).strip()\n",
    "                })\n",
    "            current_title, buf = ln, []\n",
    "        else:\n",
    "            buf.append(ln)\n",
    "\n",
    "    if current_title and any(x.strip() for x in buf):\n",
    "        entries.append({\n",
    "            \"Section Title\": current_title.strip(),\n",
    "            \"Generated Content\": \"\\n\".join(buf).strip()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(entries)\n",
    "\n",
    "# --- Compare (exact title match) ---\n",
    "results = []\n",
    "md_files = sorted(GENERATED_DIR.glob(\"*.md\"))\n",
    "print(f\"🔍 Found {len(md_files)} generated Markdown files.\")\n",
    "\n",
    "for md_file in tqdm(md_files, desc=\"📊 Comparing element-level\"):\n",
    "    # Your MD files are saved with the SAME title (sanitized) — reverse-sanitize to match Excel\n",
    "    # We’ll normalize both sides and do exact equality on normalized strings\n",
    "    gen_title_raw = md_file.stem  # e.g., \"National Institute of Mental Health (NIMH)\"\n",
    "    gen_title_norm = normalize_title(gen_title_raw)\n",
    "\n",
    "    gold_row = df_gold[df_gold[\"title_norm\"] == gen_title_norm]\n",
    "    if gold_row.empty:\n",
    "        print(f\"⚠️ No gold match for file: {md_file.name}\")\n",
    "        continue\n",
    "\n",
    "    gold_row = gold_row.iloc[0]\n",
    "    gold_title = gold_row[\"title\"]\n",
    "\n",
    "    # Gather gold element texts\n",
    "    gold_texts = {e: gold_row.get(e, \"\").strip() for e in gold_elements if gold_row.get(e, \"\").strip()}\n",
    "    if not gold_texts:\n",
    "        print(f\"⚠️ Empty gold elements for: {gold_title}\")\n",
    "        continue\n",
    "\n",
    "    # Extract sections from generated MD\n",
    "    gen_df = extract_sections(md_file)\n",
    "    if gen_df.empty:\n",
    "        print(f\"⚠️ No sections extracted from: {md_file.name}\")\n",
    "        continue\n",
    "\n",
    "    # For each gold element, compare to ALL generated sections; keep best match\n",
    "    for element, gold_text in gold_texts.items():\n",
    "        best = None\n",
    "        for _, sec in gen_df.iterrows():\n",
    "            gen_text = str(sec[\"Generated Content\"]).strip()\n",
    "            if not gen_text:\n",
    "                continue\n",
    "\n",
    "            emb_gold = sbert.encode(gold_text, convert_to_tensor=True)\n",
    "            emb_gen  = sbert.encode(gen_text,  convert_to_tensor=True)\n",
    "            sbert_sim = util.cos_sim(emb_gold, emb_gen).item()\n",
    "            rouge_l   = rouge.score(gold_text, gen_text)[\"rougeL\"].recall\n",
    "\n",
    "            cand = {\n",
    "                \"Gold Project\": gold_title,\n",
    "                \"Gold Element\": element,\n",
    "                \"Generated File\": md_file.name,\n",
    "                \"Generated Section Title\": sec[\"Section Title\"],\n",
    "                \"SBERT_Similarity\": round(sbert_sim, 4),\n",
    "                \"ROUGE_L_Recall\": round(rouge_l, 4),\n",
    "            }\n",
    "            if (best is None) or (sbert_sim > best[\"SBERT_Similarity\"]):\n",
    "                best = cand\n",
    "\n",
    "        if best:\n",
    "            results.append(best)\n",
    "\n",
    "# --- Save ---\n",
    "df_results = pd.DataFrame(results)\n",
    "out_path = EVAL_DIR / \"element_similarity_exact_titles.csv\"\n",
    "df_results.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n✅ Element-level similarity saved to: {out_path}\")\n",
    "print(f\"🧾 Total element–section best matches: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b457323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded full-document (26 rows)\n",
      "✅ Loaded element-level (312 rows)\n",
      "\n",
      "📊 Full-document summary table (Mean only, by Generated_File):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated_File</th>\n",
       "      <th>SBERT</th>\n",
       "      <th>ROUGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analysis of social media posts.md</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic Research from a Non-Human Source Example.md</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clinical Data from Human Research Participants.md</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinical and MRI data from human research part...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinical data (human biospecimens).md</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clinical data from human research participants...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Drug discovery including intellectual property.md</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gene expression analysis data from non-human m...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genomic data from a non-human source.md</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genomic data from human research participants.md</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HeLa Cell Whole Genome Sequence (DNA or RNA).md</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Clinical Trial Data.md</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Human clinical and genomic data-NIA.md</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Human clinical and genomics data.md</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Human genomic data.md</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Human survey data.md</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Non-human data (primates).md</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Non-human data (rodents)-NIA.md</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Secondary Data Analysis Example.md</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Secondary Data Analysis on Data from Human Sub...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Secondary data analysis-NIA.md</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Secondary data analysis.md</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Survey and Interview Example.md</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Survey and interview data-NIA.md</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Survey, interview, and biological data (tiered...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Technology development.md</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Generated_File SBERT ROUGE\n",
       "0                   Analysis of social media posts.md  0.79  0.43\n",
       "1   Basic Research from a Non-Human Source Example.md  0.89  0.49\n",
       "2   Clinical Data from Human Research Participants.md  0.72  0.25\n",
       "3   Clinical and MRI data from human research part...  0.70  0.29\n",
       "4               Clinical data (human biospecimens).md  0.79  0.45\n",
       "5   Clinical data from human research participants...  0.77  0.42\n",
       "6   Drug discovery including intellectual property.md  0.82  0.39\n",
       "7   Gene expression analysis data from non-human m...  0.78  0.34\n",
       "8             Genomic data from a non-human source.md  0.73  0.32\n",
       "9    Genomic data from human research participants.md  0.73  0.28\n",
       "10    HeLa Cell Whole Genome Sequence (DNA or RNA).md  0.87  0.41\n",
       "11                       Human Clinical Trial Data.md  0.65  0.27\n",
       "12             Human clinical and genomic data-NIA.md  0.83  0.46\n",
       "13                Human clinical and genomics data.md  0.61  0.43\n",
       "14                              Human genomic data.md  0.64  0.38\n",
       "15                               Human survey data.md  0.71  0.33\n",
       "16                       Non-human data (primates).md  0.71  0.35\n",
       "17                    Non-human data (rodents)-NIA.md  0.75  0.63\n",
       "18                 Secondary Data Analysis Example.md  0.74  0.43\n",
       "19  Secondary Data Analysis on Data from Human Sub...  0.77  0.42\n",
       "20                     Secondary data analysis-NIA.md  0.76  0.30\n",
       "21                         Secondary data analysis.md  0.64  0.25\n",
       "22                    Survey and Interview Example.md  0.72  0.30\n",
       "23                   Survey and interview data-NIA.md  0.73  0.30\n",
       "24  Survey, interview, and biological data (tiered...  0.77  0.39\n",
       "25                          Technology development.md  0.70  0.59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Element-level summary table (Mean ± SD):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Element</th>\n",
       "      <th>SBERT</th>\n",
       "      <th>ROUGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>element_1a</td>\n",
       "      <td>0.80 ± 0.16</td>\n",
       "      <td>0.48 ± 0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>element_1b</td>\n",
       "      <td>0.70 ± 0.11</td>\n",
       "      <td>0.45 ± 0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>element_1c</td>\n",
       "      <td>0.78 ± 0.11</td>\n",
       "      <td>0.47 ± 0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>element_2</td>\n",
       "      <td>0.79 ± 0.12</td>\n",
       "      <td>0.49 ± 0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>element_3</td>\n",
       "      <td>0.77 ± 0.14</td>\n",
       "      <td>0.49 ± 0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>element_4a</td>\n",
       "      <td>0.79 ± 0.10</td>\n",
       "      <td>0.56 ± 0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>element_4b</td>\n",
       "      <td>0.81 ± 0.09</td>\n",
       "      <td>0.49 ± 0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>element_4c</td>\n",
       "      <td>0.84 ± 0.08</td>\n",
       "      <td>0.55 ± 0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>element_5a</td>\n",
       "      <td>0.76 ± 0.11</td>\n",
       "      <td>0.48 ± 0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>element_5b</td>\n",
       "      <td>0.78 ± 0.09</td>\n",
       "      <td>0.45 ± 0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>element_5c</td>\n",
       "      <td>0.75 ± 0.14</td>\n",
       "      <td>0.43 ± 0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>element_6</td>\n",
       "      <td>0.87 ± 0.07</td>\n",
       "      <td>0.66 ± 0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Element        SBERT        ROUGE\n",
       "0   element_1a  0.80 ± 0.16  0.48 ± 0.30\n",
       "1   element_1b  0.70 ± 0.11  0.45 ± 0.24\n",
       "2   element_1c  0.78 ± 0.11  0.47 ± 0.26\n",
       "3    element_2  0.79 ± 0.12  0.49 ± 0.24\n",
       "4    element_3  0.77 ± 0.14  0.49 ± 0.30\n",
       "5   element_4a  0.79 ± 0.10  0.56 ± 0.28\n",
       "6   element_4b  0.81 ± 0.09  0.49 ± 0.21\n",
       "7   element_4c  0.84 ± 0.08  0.55 ± 0.24\n",
       "8   element_5a  0.76 ± 0.11  0.48 ± 0.27\n",
       "9   element_5b  0.78 ± 0.09  0.45 ± 0.22\n",
       "10  element_5c  0.75 ± 0.14  0.43 ± 0.28\n",
       "11   element_6  0.87 ± 0.07  0.66 ± 0.23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved formatted tables →\n",
      "• c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\evaluation_results\\summary_full_table_mean_only.csv\n",
      "• c:\\Users\\Nahid\\DMP-RAG\\data\\outputs\\evaluation_results\\summary_element_table_mean_sd.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🧮 Step 8: Summarize Evaluation Results (with Generated_File titles)\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Auto-detect project root ---\n",
    "ROOT_DIR = Path.cwd()\n",
    "while ROOT_DIR.name != \"DMP-RAG\" and ROOT_DIR.parent != ROOT_DIR:\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "\n",
    "EVAL_DIR = ROOT_DIR / \"data\" / \"outputs\" / \"evaluation_results\"\n",
    "\n",
    "# --- Load CSVs ---\n",
    "full_path = EVAL_DIR / \"full_dmp_pdf_comparison_fuzzy.csv\"\n",
    "elem_path = EVAL_DIR / \"element_similarity_exact_titles.csv\"\n",
    "\n",
    "df_full = pd.read_csv(full_path)\n",
    "df_elem = pd.read_csv(elem_path)\n",
    "\n",
    "print(f\"✅ Loaded full-document ({len(df_full)} rows)\")\n",
    "print(f\"✅ Loaded element-level ({len(df_elem)} rows)\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 🧩 1️⃣ FULL-DOCUMENT LEVEL SUMMARY (Mean Only, by Generated_File)\n",
    "# ============================================================\n",
    "\n",
    "# Prefer \"Generated_File\" column; fallback to detected one\n",
    "if \"Generated_File\" in df_full.columns:\n",
    "    project_col = \"Generated_File\"\n",
    "else:\n",
    "    project_col = next(\n",
    "        (c for c in df_full.columns if \"title\" in c.lower() or \"project\" in c.lower() or \"matched\" in c.lower()),\n",
    "        df_full.columns[0],\n",
    "    )\n",
    "\n",
    "# Find numeric columns\n",
    "numeric_cols = [c for c in df_full.columns if \"sbert\" in c.lower() or \"rouge\" in c.lower()]\n",
    "\n",
    "# Compute mean per file (if multiple rows)\n",
    "df_full_summary = (\n",
    "    df_full.groupby(project_col)[numeric_cols]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Format to 2 decimals\n",
    "df_full_summary[\"SBERT\"] = df_full_summary[numeric_cols[0]].apply(lambda x: f\"{x:.2f}\")\n",
    "df_full_summary[\"ROUGE\"] = df_full_summary[numeric_cols[1]].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# Reorder columns and rename for clarity\n",
    "df_full_table = df_full_summary[[project_col, \"SBERT\", \"ROUGE\"]].rename(\n",
    "    columns={project_col: \"Generated_File\"}\n",
    ")\n",
    "\n",
    "print(\"📊 Full-document summary table (Mean only, by Generated_File):\")\n",
    "display(df_full_table)\n",
    "\n",
    "# ============================================================\n",
    "# 🧩 2️⃣ ELEMENT-LEVEL SUMMARY (Mean ± SD)\n",
    "# ============================================================\n",
    "\n",
    "elem_col = next(\n",
    "    (c for c in df_elem.columns if \"element\" in c.lower()),\n",
    "    df_elem.columns[0],\n",
    ")\n",
    "\n",
    "numeric_cols_elem = [c for c in df_elem.columns if \"sbert\" in c.lower() or \"rouge\" in c.lower()]\n",
    "df_elem_summary = (\n",
    "    df_elem.groupby(elem_col)[numeric_cols_elem]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "flat_cols_elem = [elem_col, \"SBERT_Mean\", \"SBERT_SD\", \"ROUGE_Mean\", \"ROUGE_SD\"]\n",
    "df_elem_summary.columns = flat_cols_elem\n",
    "\n",
    "df_elem_summary[\"SBERT\"] = df_elem_summary.apply(\n",
    "    lambda r: f\"{r['SBERT_Mean']:.2f} ± {r['SBERT_SD']:.2f}\", axis=1)\n",
    "df_elem_summary[\"ROUGE\"] = df_elem_summary.apply(\n",
    "    lambda r: f\"{r['ROUGE_Mean']:.2f} ± {r['ROUGE_SD']:.2f}\", axis=1)\n",
    "\n",
    "df_elem_table = df_elem_summary[[elem_col, \"SBERT\", \"ROUGE\"]].rename(\n",
    "    columns={elem_col: \"Element\"}\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Element-level summary table (Mean ± SD):\")\n",
    "display(df_elem_table)\n",
    "\n",
    "# ============================================================\n",
    "# 💾 Save formatted tables\n",
    "# ============================================================\n",
    "out_full = EVAL_DIR / \"summary_full_table_mean_only.csv\"\n",
    "out_elem = EVAL_DIR / \"summary_element_table_mean_sd.csv\"\n",
    "\n",
    "df_full_table.to_csv(out_full, index=False)\n",
    "df_elem_table.to_csv(out_elem, index=False)\n",
    "\n",
    "print(f\"\\n💾 Saved formatted tables →\\n• {out_full}\\n• {out_elem}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
